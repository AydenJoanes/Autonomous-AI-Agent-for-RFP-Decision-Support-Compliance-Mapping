## **UPDATED PHASE 4 IMPLEMENTATION PLAN**

---

## **NEW ARCHITECTURE**

\[6 LangChain Tools\]

        ↓

\[Each returns: status, compliance\_level, confidence, details\]

        ↓

\[ComplianceStrategy maps statuses → unified compliance\]

        ↓

\[Phase 5 aggregates using unified compliance levels\]

---

## **STANDARDIZED OUTPUT FORMAT (ALL TOOLS)**

{

  "tool\_name": str,                    \# Which tool ran

  "requirement": str,                  \# What was checked

  "status": str,                       \# Tool-specific status

  "compliance\_level": str,             \# COMPLIANT | NON\_COMPLIANT | PARTIAL | UNKNOWN | WARNING

  "confidence": float,                 \# 0.0 \- 1.0

  "details": dict,                     \# Tool-specific details

  "risks": List\[str\],                  \# Risk factors identified

  "message": str,                      \# Human-readable explanation

  "timestamp": datetime                \# When checked

}

---

## **COMPLIANCE LEVEL ENUM (STANDARDIZED)**

class ComplianceLevel(str, Enum):

    COMPLIANT \= "COMPLIANT"              \# Fully meets requirement

    NON\_COMPLIANT \= "NON\_COMPLIANT"      \# Does not meet requirement

    PARTIAL \= "PARTIAL"                  \# Partially meets requirement

    UNKNOWN \= "UNKNOWN"                  \# Cannot determine (data missing)

    WARNING \= "WARNING"                  \# Meets but with concerns (expiring, stale, risky)

---

## **UPDATED IMPLEMENTATION STEPS**

---

### **STEP 1: Create Standard Types and Enums**

**File:** `app/models/compliance.py`

**Purpose:** Define standard types for all tools

**What It Contains:**

**ComplianceLevel Enum:**

* COMPLIANT  
* NON\_COMPLIANT  
* PARTIAL  
* UNKNOWN  
* WARNING

**ToolResult Class (Pydantic):**

* tool\_name: str  
* requirement: str  
* status: str (tool-specific)  
* compliance\_level: ComplianceLevel  
* confidence: float (0.0-1.0, validated)  
* details: dict  
* risks: List\[str\]  
* message: str  
* timestamp: datetime (auto-generated)

**Validators:**

* confidence must be 0.0-1.0  
* timestamp auto-set  
* compliance\_level must be valid enum

---

### **STEP 2: Knowledge Query Tool (LangChain)**

**File:** `app/agent/tools/knowledge_query_tool.py`

**Class: KnowledgeQueryTool**

**Extends:** `langchain.tools.BaseTool` (**NOT custom BaseTool**)

**LangChain Required Attributes:**

* `name`: "knowledge\_query" (must be unique)  
* `description`: "Searches past project portfolio for similar projects using semantic search. Input should be a requirement text with embedding."

**Initialization:**

* Initialize ProjectRepository  
* Initialize logger  
* Store database session

**Method: `_run(requirement_text: str, requirement_embedding: List[float]) -> str`**

**Important: LangChain \_run returns STRING, not dict**

**Logic:**

**Step 1: Search similar projects**

* Use requirement\_embedding for vector search  
* Query ProjectRepository.search\_similar()  
* Apply similarity threshold (0.7)

**Step 2: Determine compliance and confidence**

* If found \>= 3 similar successful projects:  
  * compliance\_level \= COMPLIANT  
  * confidence \= average\_similarity\_score  
* If found 1-2 similar projects:  
  * compliance\_level \= PARTIAL  
  * confidence \= 0.6  
* If found similar projects but all failed:  
  * compliance\_level \= WARNING  
  * confidence \= 0.7  
* If no similar projects found:  
  * compliance\_level \= UNKNOWN  
  * confidence \= 0.3

**Step 3: Build ToolResult**

* Create ToolResult object  
* status \= f"Found {count} similar projects"  
* Include all required fields

**Step 4: Return as JSON string**

* LangChain expects string return  
* Convert ToolResult to JSON string  
* Return JSON string

**Error Handling:**

* Try-catch around database query  
* If error: Return ToolResult with compliance\_level=UNKNOWN, confidence=0.0

---

### 

### **STEP 3: Certification Checker Tool (LangChain)**

**File:** `app/agent/tools/certification_checker_tool.py`

**Class: CertificationCheckerTool**

**Extends:** `langchain.tools.BaseTool`

**LangChain Required Attributes:**

* `name`: "certification\_checker"  
* `description`: "Checks if company has required certification. Input should be certification name."

**Method: `_run(certification_name: str) -> str`**

**Logic:**

**Step 1: Search certification**

* Normalize name (case-insensitive, trim)  
* Query CertificationRepository.get\_by\_name()  
* Try exact match, then fuzzy match

**Step 2: Determine status and compliance**

* **If not found:**

  * status \= "NOT\_FOUND"  
  * compliance\_level \= UNKNOWN (not NON\_COMPLIANT\!)  
  * confidence \= 0.9 (high confidence it's not in DB, but maybe we have it and didn't record)  
  * message \= "Certification not found in database. Manual verification needed."  
* **If found, status \= "active":**

  * Check expiry date  
  * If valid \> 12 months: compliance\_level \= COMPLIANT, confidence \= 1.0  
  * If valid 3-12 months: compliance\_level \= WARNING, confidence \= 1.0  
  * If valid \< 3 months: compliance\_level \= WARNING, confidence \= 1.0  
  * risks \= \["Expiring in X days"\]  
* **If found, status \= "expired":**

  * compliance\_level \= NON\_COMPLIANT  
  * confidence \= 1.0  
* **If found, status \= "pending":**

  * compliance\_level \= PARTIAL  
  * confidence \= 0.7

**Step 3: Fuzzy match handling**

* If fuzzy matched (not exact):  
  * Reduce confidence by 0.2  
  * Add to message: "Matched similar certification: {matched\_name}"

**Step 4: Return ToolResult as JSON string**

---

### **STEP 4: Tech Validator Tool (LangChain)**

**File:** `app/agent/tools/tech_validator_tool.py`

**Class: TechValidatorTool**

**Extends:** `langchain.tools.BaseTool`

**LangChain Required Attributes:**

* `name`: "tech\_validator"  
* `description`: "Validates company technology expertise. Input should be technology name."

**Method: `_run(technology: str) -> str`**

**Logic:**

**Step 1: Search technology**

* Normalize name  
* Query TechStackRepository.get\_by\_name()  
* Try variants (Python \= python \= Python3)

**Step 2: Determine status and compliance**

* **If not found:**

  * status \= "NOT\_IN\_DATABASE"  
  * compliance\_level \= UNKNOWN (not NON\_COMPLIANT\!)  
  * confidence \= 0.8  
  * message \= "Technology not in official stack. Team might still have expertise."  
* **If found:**

  * Check proficiency level  
  * Check team size  
  * Check last\_used date  
* **Proficiency assessment:**

  * expert: compliance\_level \= COMPLIANT, confidence \= 1.0  
  * advanced: compliance\_level \= COMPLIANT, confidence \= 0.9  
  * intermediate: compliance\_level \= PARTIAL, confidence \= 0.7  
  * beginner: compliance\_level \= PARTIAL, confidence \= 0.5  
* **Recency check:**

  * last\_used \< 1 year: confidence unchanged  
  * last\_used 1-2 years: reduce confidence by 0.1, add WARNING  
  * last\_used 2-5 years: reduce confidence by 0.2, compliance\_level \= WARNING  
  * last\_used \> 5 years: reduce confidence by 0.3, compliance\_level \= PARTIAL

**Step 3: Return ToolResult as JSON string**

---

### **STEP 5: Budget Analyzer Tool (LangChain)**

**File:** `app/agent/tools/budget_analyzer_tool.py`

**Class: BudgetAnalyzerTool**

**Extends:** `langchain.tools.BaseTool`

**LangChain Required Attributes:**

* `name`: "budget\_analyzer"  
* `description`: "Analyzes RFP budget feasibility. Input should be budget amount in USD."

**Method: `_run(budget: str) -> str`**

**Note:** LangChain passes strings, must parse to int

**Logic:**

**Step 1: Parse budget**

* Convert string to int  
* Handle formats: "$150,000" or "150000"

**Step 2: Get company capacity**

* Query CompanyProfile for budget\_capacity\_min, budget\_capacity\_max

**Step 3: Determine status and compliance**

* **If budget \< budget\_capacity\_min:**

  * status \= "BELOW\_MINIMUM"  
  * compliance\_level \= WARNING (might accept if strategic)  
  * confidence \= 0.8  
  * message \= "Budget below typical minimum. Low priority."  
* **If budget \> budget\_capacity\_max:**

  * status \= "EXCEEDS\_MAXIMUM"  
  * compliance\_level \= NON\_COMPLIANT  
  * confidence \= 0.9  
  * message \= "Budget exceeds company capacity."  
* **If within range:**

  * Calculate percentile  
  * If 0-25%: compliance \= COMPLIANT, status \= "LOW\_END", confidence \= 0.9  
  * If 25-75%: compliance \= COMPLIANT, status \= "ACCEPTABLE", confidence \= 1.0  
  * If 75-100%: compliance \= WARNING, status \= "HIGH\_END", confidence \= 0.8  
  * risks \= \["Large budget may strain capacity"\]

**Step 4: Compare with historical projects**

* Query similar projects  
* If no historical data: reduce confidence by 0.2  
* If significantly different from historical avg: add risk

**Step 5: Return ToolResult as JSON string**

---

### **STEP 6: Timeline Assessor Tool (LangChain)**

**File:** `app/agent/tools/timeline_assessor_tool.py`

**Class: TimelineAssessorTool**

**Extends:** `langchain.tools.BaseTool`

**LangChain Required Attributes:**

* `name`: "timeline\_assessor"  
* `description`: "Assesses timeline feasibility. Input should be timeline in months."

**Method: `_run(timeline: str) -> str`**

**Logic:**

**Step 1: Parse timeline**

* Extract months from string  
* Handle formats: "4 months", "4", "16 weeks"

**Step 2: Get current capacity**

* Query CompanyProfile for team\_size  
* Hardcode current\_utilization \= 0.70 (for now)  
* Calculate available capacity

**Step 3: Query historical projects**

* Find similar projects (same industry/complexity)  
* Get average duration, min, max

**Step 4: Determine status and compliance**

* **If no historical data:**

  * compliance\_level \= UNKNOWN  
  * confidence \= 0.4  
  * status \= "NO\_HISTORICAL\_DATA"  
* **If timeline within historical range (min to max):**

  * compliance\_level \= COMPLIANT  
  * If near average: confidence \= 0.9  
  * If at extremes: confidence \= 0.7  
* **If timeline \< historical min (aggressive):**

  * If \< 80% of min: compliance \= WARNING, confidence \= 0.6  
  * If \< 50% of min: compliance \= NON\_COMPLIANT, confidence \= 0.8  
  * status \= "AGGRESSIVE"  
* **If timeline \> 150% of historical average:**

  * compliance \= COMPLIANT  
  * status \= "CONSERVATIVE"  
  * confidence \= 1.0

**Step 5: Capacity check**

* If team utilization \> 80%: reduce confidence by 0.1, add risk

**Step 6: Return ToolResult as JSON string**

---

### **STEP 7: Strategy Evaluator Tool (LangChain)**

**File:** `app/agent/tools/strategy_evaluator_tool.py`

**Class: StrategyEvaluatorTool**

**Extends:** `langchain.tools.BaseTool`

**LangChain Required Attributes:**

* `name`: "strategy\_evaluator"  
* `description`: "Evaluates RFP strategic alignment. Input should be JSON with: industry, technologies, project\_type, client\_sector."

**Method: `_run(rfp_context: str) -> str`**

**Note:** Input is JSON string, must parse

**Logic:**

**Step 1: Parse input**

* Parse JSON string to dict  
* Extract: industry, technologies, project\_type, client\_sector

**Step 2: Query strategic preferences**

* Get all preferences from database  
* Group by type

**Step 3: Calculate dimension scores**

* Industry score (0-1)  
* Technology score (0-1)  
* Project type score (0-1)  
* Client sector score (0-1)

**Step 4: Calculate composite score**

* Weighted average (0-100)  
* Industry: 30%, Technology: 25%, Project type: 25%, Sector: 10%, Other: 10%

**Step 5: Determine compliance and confidence**

* **Score \>= 80:**

  * compliance\_level \= COMPLIANT  
  * status \= "STRONG\_ALIGNMENT"  
  * confidence \= score / 100  
* **Score 60-79:**

  * compliance\_level \= PARTIAL  
  * status \= "MODERATE\_ALIGNMENT"  
  * confidence \= 0.7  
* **Score 40-59:**

  * compliance\_level \= WARNING  
  * status \= "WEAK\_ALIGNMENT"  
  * confidence \= 0.6  
* **Score \< 40:**

  * compliance\_level \= NON\_COMPLIANT  
  * status \= "MISALIGNMENT"  
  * confidence \= 0.8

**Step 6: Generate strategic flags**

* "CORE\_COMPETENCY", "STRATEGIC\_EXPANSION", "OFF\_STRATEGY"

**Step 7: Return ToolResult as JSON string**

---

### **STEP 8: Compliance Strategy (Mapping)**

**File:** `app/strategies/compliance_strategy.py`

**Purpose:** Map tool-specific statuses to unified ComplianceLevel

**Function: `map_status_to_compliance(tool_name: str, status: str) -> ComplianceLevel`**

**Mapping rules:**

**Certification Checker:**

* "VALID" → COMPLIANT  
* "EXPIRING\_SOON" → WARNING  
* "EXPIRED" → NON\_COMPLIANT  
* "PENDING" → PARTIAL  
* "NOT\_FOUND" → UNKNOWN

**Tech Validator:**

* "AVAILABLE" (expert/advanced) → COMPLIANT  
* "AVAILABLE" (intermediate) → PARTIAL  
* "AVAILABLE" (beginner) → PARTIAL  
* "NOT\_IN\_DATABASE" → UNKNOWN  
* "STALE" → WARNING  
* "OUTDATED" → PARTIAL

**Budget Analyzer:**

* "ACCEPTABLE" → COMPLIANT  
* "LOW\_END" → COMPLIANT  
* "HIGH\_END" → WARNING  
* "BELOW\_MINIMUM" → WARNING  
* "EXCEEDS\_MAXIMUM" → NON\_COMPLIANT

**Timeline Assessor:**

* "FEASIBLE" → COMPLIANT  
* "TIGHT" → WARNING  
* "AGGRESSIVE" → WARNING  
* "UNREALISTIC" → NON\_COMPLIANT  
* "NO\_HISTORICAL\_DATA" → UNKNOWN

**Strategy Evaluator:**

* "STRONG\_ALIGNMENT" → COMPLIANT  
* "MODERATE\_ALIGNMENT" → PARTIAL  
* "WEAK\_ALIGNMENT" → WARNING  
* "MISALIGNMENT" → NON\_COMPLIANT

**Function: `aggregate_compliance(tool_results: List[ToolResult]) -> dict`**

**Returns:**

{

  "overall\_compliance": ComplianceLevel,

  "compliant\_count": int,

  "non\_compliant\_count": int,

  "partial\_count": int,

  "unknown\_count": int,

  "warning\_count": int,

  "confidence\_avg": float,

  "mandatory\_requirements\_met": bool

}

**Logic:**

* If any mandatory requirement is NON\_COMPLIANT → overall \= NON\_COMPLIANT  
* If all COMPLIANT → overall \= COMPLIANT  
* If mix of COMPLIANT \+ PARTIAL → overall \= PARTIAL  
* If any UNKNOWN on mandatory → overall \= UNKNOWN  
* Weighted by confidence scores

---

### **STEP 9: Test Script (Updated)**

**File:** `scripts/test_reasoning_tools.py`

**Tests Each Tool with:**

* Valid input (expected pass)  
* Invalid input (expected fail/unknown)  
* Edge cases (expected warning/partial)  
* Missing data (expected unknown)

**For each test, verify:**

* Returns valid JSON string  
* Contains all required ToolResult fields  
* compliance\_level is valid enum  
* confidence is 0.0-1.0  
* status is appropriate  
* message is human-readable

**Print summary table:**

Tool                    | Test Case           | Compliance  | Confidence | Status

\--------------------------------------------------------------------------------

knowledge\_query         | Similar projects    | COMPLIANT   | 0.85       | PASS

knowledge\_query         | No similar projects | UNKNOWN     | 0.30       | PASS

certification\_checker   | Found & valid       | COMPLIANT   | 1.00       | PASS

certification\_checker   | Not found           | UNKNOWN     | 0.90       | PASS

certification\_checker   | Expired             | NON\_COMPLIANT| 1.00      | PASS

...

---

## **UPDATED SEQUENTIAL CHECKLIST**

---

### **✅ STEP 1: Standard Types**

* \[ \] Create `app/models/compliance.py`  
* \[ \] Define ComplianceLevel enum (5 values including UNKNOWN)  
* \[ \] Define ToolResult Pydantic model  
* \[ \] Add validators (confidence 0-1, auto timestamp)  
* \[ \] Test model instantiation

---

### **✅ STEP 2: Knowledge Query Tool**

* \[ \] Create `app/agent/tools/knowledge_query_tool.py`  
* \[ \] Import `from langchain.tools import BaseTool`  
* \[ \] Define class extending LangChain BaseTool  
* \[ \] Set name \= "knowledge\_query"  
* \[ \] Set description for LLM  
* \[ \] Implement `_run(requirement_text, requirement_embedding)` → returns JSON string  
* \[ \] Search similar projects with vector search  
* \[ \] Determine compliance\_level based on results  
* \[ \] Calculate confidence based on similarity scores  
* \[ \] Handle UNKNOWN case (no similar projects)  
* \[ \] Return ToolResult as JSON string  
* \[ \] Test with sample requirement  
* \[ \] Verify returns valid JSON  
* \[ \] Verify confidence always 0-1

---

### **✅ STEP 3: Certification Checker Tool**

* \[ \] Create `app/agent/tools/certification_checker_tool.py`  
* \[ \] Extend LangChain BaseTool  
* \[ \] Set name \= "certification\_checker"  
* \[ \] Implement `_run(certification_name)` → JSON string  
* \[ \] Search certification in database  
* \[ \] If not found: Return compliance=UNKNOWN (not NON\_COMPLIANT)  
* \[ \] If found: Check validity, expiry  
* \[ \] Handle fuzzy matching with reduced confidence  
* \[ \] Return ToolResult as JSON string  
* \[ \] Test: Found & valid → COMPLIANT, confidence 1.0  
* \[ \] Test: Not found → UNKNOWN, confidence 0.9  
* \[ \] Test: Expired → NON\_COMPLIANT, confidence 1.0  
* \[ \] Test: Expiring soon → WARNING, confidence 1.0

---

### **✅ STEP 4: Tech Validator Tool**

* \[ \] Create `app/agent/tools/tech_validator_tool.py`  
* \[ \] Extend LangChain BaseTool  
* \[ \] Set name \= "tech\_validator"  
* \[ \] Implement `_run(technology)` → JSON string  
* \[ \] Search technology in database  
* \[ \] If not found: Return compliance=UNKNOWN  
* \[ \] If found: Check proficiency, recency  
* \[ \] Adjust confidence based on proficiency level  
* \[ \] Reduce confidence if stale (\> 2 years)  
* \[ \] Return ToolResult as JSON string  
* \[ \] Test: Expert proficiency → COMPLIANT, confidence 1.0  
* \[ \] Test: Not in DB → UNKNOWN, confidence 0.8  
* \[ \] Test: Stale (5 years) → PARTIAL, confidence 0.7

---

### **✅ STEP 5: Budget Analyzer Tool**

* \[ \] Create `app/agent/tools/budget_analyzer_tool.py`  
* \[ \] Extend LangChain BaseTool  
* \[ \] Set name \= "budget\_analyzer"  
* \[ \] Implement `_run(budget)` → JSON string (parse string to int)  
* \[ \] Query company budget capacity  
* \[ \] Calculate percentile position  
* \[ \] Determine compliance based on range  
* \[ \] Query historical projects if available  
* \[ \] If no historical data: Reduce confidence  
* \[ \] Return ToolResult as JSON string  
* \[ \] Test: Within range → COMPLIANT  
* \[ \] Test: Above max → NON\_COMPLIANT  
* \[ \] Test: Below min → WARNING

---

### **✅ STEP 6: Timeline Assessor Tool**

* \[ \] Create `app/agent/tools/timeline_assessor_tool.py`  
* \[ \] Extend LangChain BaseTool  
* \[ \] Set name \= "timeline\_assessor"  
* \[ \] Implement `_run(timeline)` → JSON string (parse months)  
* \[ \] Query historical project durations  
* \[ \] If no historical data: Return UNKNOWN  
* \[ \] Compare timeline to historical range  
* \[ \] Assess team capacity  
* \[ \] Determine compliance and confidence  
* \[ \] Return ToolResult as JSON string  
* \[ \] Test: Within historical range → COMPLIANT  
* \[ \] Test: Very aggressive → NON\_COMPLIANT  
* \[ \] Test: No historical data → UNKNOWN

---

### **✅ STEP 7: Strategy Evaluator Tool**

* \[ \] Create `app/agent/tools/strategy_evaluator_tool.py`  
* \[ \] Extend LangChain BaseTool  
* \[ \] Set name \= "strategy\_evaluator"  
* \[ \] Implement `_run(rfp_context_json)` → JSON string (parse JSON input)  
* \[ \] Query strategic preferences  
* \[ \] Calculate dimension scores (industry, tech, type, sector)  
* \[ \] Calculate weighted composite score  
* \[ \] Determine compliance from score  
* \[ \] Set confidence \= score / 100  
* \[ \] Return ToolResult as JSON string  
* \[ \] Test: High score (85) → COMPLIANT, confidence 0.85  
* \[ \] Test: Low score (35) → NON\_COMPLIANT, confidence 0.8

---

### **✅ STEP 8: Compliance Strategy**

* \[ \] Create `app/strategies/compliance_strategy.py`  
* \[ \] Implement `map_status_to_compliance(tool_name, status)`  
* \[ \] Create mapping dictionary for each tool  
* \[ \] Handle all possible statuses  
* \[ \] Default to UNKNOWN if unmapped status  
* \[ \] Implement `aggregate_compliance(tool_results)`  
* \[ \] Count by compliance\_level  
* \[ \] Calculate confidence average  
* \[ \] Check mandatory requirements  
* \[ \] Determine overall compliance  
* \[ \] Test with sample tool results  
* \[ \] Verify aggregation logic

---

### **✅ STEP 9: Testing**

* \[ \] Create `scripts/test_reasoning_tools.py`  
* \[ \] Import all 6 tools  
* \[ \] Import ToolResult, ComplianceLevel  
* \[ \] Import JSON for parsing  
* \[ \] Test each tool with 3-4 scenarios  
* \[ \] Parse JSON string responses  
* \[ \] Validate ToolResult structure  
* \[ \] Check compliance\_level is valid enum  
* \[ \] Check confidence is 0.0-1.0  
* \[ \] Verify UNKNOWN handled correctly  
* \[ \] Print summary table  
* \[ \] Count pass/fail  
* \[ \] Flag any errors

---

### **✅ STEP 10: Integration Prep**

* \[ \] Create `app/agent/tools/__init__.py`  
* \[ \] Import all 6 LangChain tools  
* \[ \] Export list: REASONING\_TOOLS \= \[tool1, tool2, ...\]  
* \[ \] Create `docs/PHASE4.md`  
* \[ \] Document LangChain integration approach  
* \[ \] Document ToolResult standard format  
* \[ \] Document ComplianceLevel enum  
* \[ \] Document confidence scoring approach  
* \[ \] Document UNKNOWN handling  
* \[ \] Add examples for each tool  
* \[ \] Add integration notes for Phase 5

---

## **VERIFICATION CHECKLIST**

### **✅ LangChain Integration:**

* \[ \] All tools extend `langchain.tools.BaseTool`  
* \[ \] All tools have `name` attribute  
* \[ \] All tools have `description` attribute  
* \[ \] All tools implement `_run()` method  
* \[ \] All `_run()` methods return JSON string  
* \[ \] Tools can be imported and instantiated  
* \[ \] Tools work with LangChain agent (Phase 5 prep)

### **✅ Standardized Outputs:**

* \[ \] All tools return ToolResult as JSON string  
* \[ \] All include compliance\_level (from enum)  
* \[ \] All include confidence (0.0-1.0)  
* \[ \] All include status (tool-specific)  
* \[ \] All include message (human-readable)  
* \[ \] All include timestamp

### **✅ UNKNOWN Handling:**

* \[ \] Certification not found → UNKNOWN (not NON\_COMPLIANT)  
* \[ \] Tech not in DB → UNKNOWN (not NON\_COMPLIANT)  
* \[ \] No historical data → UNKNOWN  
* \[ \] Ambiguous data → UNKNOWN  
* \[ \] Confidence appropriately reflects uncertainty

### **✅ Confidence Scoring:**

* \[ \] All tools include confidence value  
* \[ \] Confidence always 0.0-1.0  
* \[ \] Confidence reflects certainty appropriately  
* \[ \] High confidence (1.0) for exact matches  
* \[ \] Medium confidence (0.7-0.8) for fuzzy matches  
* \[ \] Low confidence (0.4-0.6) for inferences  
* \[ \] Very low confidence (\< 0.4) for unknowns

### **✅ Compliance Mapping:**

* \[ \] All statuses mapped to ComplianceLevel  
* \[ \] Mapping function handles all statuses  
* \[ \] Aggregation logic accounts for confidence  
* \[ \] Mandatory vs optional requirements distinguished  
* \[ \] Overall compliance calculated correctly

---

## **PHASE 4 COMPLETE WHEN:**

✅ All 6 tools extend LangChain BaseTool  
 ✅ All tools return standardized ToolResult as JSON string  
 ✅ All tools include compliance\_level, confidence, status  
 ✅ UNKNOWN status properly handled  
 ✅ Confidence scores meaningful and validated  
 ✅ Compliance mapping standardized  
 ✅ Test script passes all scenarios  
 ✅ Documentation complete  
 ✅ Ready for Phase 5 LangChain ReAct agent integration

---

**Key changes from original plan:**

1. ✅ Use LangChain BaseTool (not custom)  
2. ✅ Add confidence to all outputs  
3. ✅ Add UNKNOWN status everywhere  
4. ✅ Standardize compliance mapping  
5. ✅ Return JSON strings (LangChain requirement)

**This updated plan is production-ready and addresses all feedback.**